<!DOCTYPE html>
<head>
    <link rel="stylesheet" href="bod.css">
</head>
<html>
<div class="topnav">
    <a href="./index.html">Home</a>
    <a href="./schedule.html">Schedule</a>
    <a href="./reading.html">Reading</a>
    <a href="./participants.html">Participants</a>
    <a href="./logistics.html">Logistics</a>
    <a href="./contact.html">Contact</a>
  </div> 
<body>

<H1>The Blessing of Dimensionality: Theory and Applications</H1>

<h2>A Summer Workshop at the University of Connecticut</h2>
<h3>July 15-18, 2024</h3>
<div>
<p>
<b>Abstract</b>: Over the past decade, data analyses have drastically changed with the 
development of high-dimensional statistical inference. Indeed, on each individual, more 
and more features are measured to the point that their number usually far exceeds the 
number of observations. For instance, this is the case in biology, specifically in 
microbiology, where millions of genes, proteins, and metabolites are measured for a single 
individual. High-resolution imaging, finance, online advertising, and climate studies, 
to name a few areas, are among the fields processing vast amounts of data. The classical 
theories and software tools developed for the large N (sample size) small P (number of feature)
scenarios are of little use in the age of Big Data. These classical approaches suffer 
from the so-called “curse of dimensionality” – suggesting that collecting more information
on each subject in a sample worsens the ability to yield good predictions. It turns out
that in very high-dimensional spaces, the signal can be more easily separated from the noise.
This is because, in high dimensional spaces, the noise becomes easily detectible – it turns 
out that noisy data in high-dimensional spaces has some well-defined, quantifiable properties. 
Once we determine what the noise is, the signal becomes apparent. The theories that this 
application will focus on rely on convex geometry in high dimensional Euclidean spaces, 
where one can take advantage of the so-called concentration of measure property. This property 
means that, even when the dimension of the space is very large, the main features can be 
described in terms of a much smaller subspace. Concentration of measure is related to the 
well-known theory of shrinkage estimation in the statistical literature, where one gains 
power by borrowing information across multiple predictors in order to improve the estimation 
of parameters in models of interest.
</p>
</div>

<div>
    <p>
    
<b>Workshop objective</b>: Developing a new approach to large optimization problems requires 
an interdisciplinary effort, close collaborations, and open discussions among experts from 
multiple areas. In this workshop, several experts from across the country will have a golden 
opportunity to discuss the fascinating theory that underlies the concentration of measure 
phenomenon, and its potential applications. The main goals of this workshop are to spur new 
collaborations that can lead to breakthroughs in Big Data analysis and to solving very large 
optimization problems using methods built on sturdy mathematical foundations, and to introduce 
this exciting research area to early-career mathematicians, statisticians, and computer scientists.

</p>
</div>

<div>
    <p>This workshop is funded by the College of Liberal Arts and Sciences (Research in Academic Themes, 2024), and by BlackRock Inc. 
    </p>
</div>


</body>


</html>