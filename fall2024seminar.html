<!DOCTYPE html>
<head>
<title>Joint seminar -- University of California, Santa Barbara -
 University of California, Berkeley -
Florida State University --
University of Connecticut</title>
    <link rel="stylesheet" href="bod.css">
</head>
<html>
    <body>
<H1>Fall 2024 Joint Seminar</H1>
<center>University of California, Santa Barbara<br>
University of California, Berkeley<br>
Florida State University<br>
University of Connecticut<br>
 <p>Organizers: Alex Shkolnik, Lisa Goldberg, Alec Kercheval, Haim Bar</p>
 <p>Coordinator: Gevorg Khandamiryan, gevorgkh@berkeley.edu</p>
</center> 
<div>
 <p>
  <b>Description</b>: We focus on applications of high-dimensional data analysis to finance, machine learning,
 genomics, neuroscience and engineering.
The seminar will be held across four universities (UC Berkeley, UC Santa Barbara, Florida State University, and UConn),
 with lectures by students, faculty, and guest speakers. Students are expected to participate in-person 
 in the weekly cross-campus meetings, which will be connected via zoom. The formal presentation will start 
 at 4:10pm ET / 1:10pm PT, and we will continue with informal discussions and exploration of possibilities for future research. 
 Students are encouraged to join collaborative projects emphasizing studies with empirical and simulated data, 
 and to write up and present their own research.
 </p>
<p>
<b>Zoom link</b>: <a href="https://ucsb.zoom.us/my/shkolnik">https://ucsb.zoom.us/my/shkolnik</a>
 
<b>Discord link</b>: <a href="https://discord.gg/3x2Rkn8F">https://discord.gg/3x2Rkn8F</a>

<p>
<b>Meeting time</b>: Friday, 4:10pm ET / 1:10pm PT via Zoom.

<p>
<b>Locations</b>:
<br>
UConn: AUST 344
<br>
FSU: LOV 102
<br>
UCSB
<br>
UC Berkeley: Evans 639. In-person meeting at 2:10pm PT.
</p>

 <p>
  <b>Schedule: </b>
  <ul>
   <li>
    Aug 30: Organizational meeting and introduction.
  <a href="http://haimbar.github.io/BlessingOfDimensionalityWorkshop/files/sem217.pptx">Slides</a>
   </li>
   <li>
    Sep 6: <ul>
        <li>Improving quadratic optimization inputs with techniques with concentration of measure (Lisa)</li>
        <li>Variance concentration and average pairwise correlation in index covariance matrices (Jacob L)</li>
        <li>Project groups (sign up  by Friday 13 September, <a href="https://docs.google.com/spreadsheets/d/1g59g6xkyet0Q75qjmQwaAs4VlU1EpDfeQcuA2LTbdWQ/edit?usp=sharing">link</a>)</li>
        <li>Optimization and long-only constraints (Alex B - if there’s time. If not, first thing next time)</li>
           </ul>
   </li>
   <li>
    Sept 13:
   </li>
   <li>Sept 20:
   </li>
  </ul>
 </p>

 

 <p>
  <b>Reading list: </b>
  <ul>
   <li>
    <b>Concentration of measure:  background for graphical models, James Stein for Eigenvectors and everything else</b>: 
    <br><a href="https://home.ttic.edu/~avrim/book.pdf">Foundations of Data Science</a>
    <br><a href="https://www.amazon.com/Geometric-Analysis-Mathematical-Institute-Publications/dp/0521642590">Convex Geometric Analysis</a>
    <br><a href="https://terrytao.wordpress.com/2015/10/23/275a-notes-3-the-weak-and-strong-law-of-large-numbers/">The weak and strong law of large numbers</a>
    <br><a href="https://terrytao.wordpress.com/2010/01/03/254a-notes-1-concentration-of-measure/">Concentration of measure</a>
    <br><a href="http://haimbar.github.io/BlessingOfDimensionalityWorkshop/files/talagrand1966.pdf">A New Look at Independence</a>
   </li>
   <li>
    <b>Machine learning and high dimensional statistics:</b>: 
    <br><a href="https://www.pnas.org/doi/full/10.1073/pnas.1903070116">Reconciling modern machine-learning practice and the classical bias–variance trade-off</a>
   </li>
   <li>
    <b>James Stein for Eigenvectors / Optimization</b>: 
    <br><a href="https://epubs.siam.org/doi/10.1137/21M144058X">The Dispersion Bias</a>
    <br><a href="https://www.pm-research.com/content/iijpormgmt/47/1/119">Better Betas</a>
    <br><a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/sta4.419">James–Stein estimation of the first principal component</a>
    <br><a href="https://www.pnas.org/doi/abs/10.1073/pnas.2207046120">James–Stein for the leading eigenvector</a>
    <br><a href="https://cdar.berkeley.edu/publications/portfolio-optimization-strategy-specific-eigenvector-shrinkage">Portfolio optimization via strategy-specific eigenvector shrinkage</a>
    <br><a href="https://cdar.berkeley.edu/sites/default/files/publications/portfolio_selection_revisited.pdf">Portfolio Selection Revisited
in memory of Harry Markowitz</a>
   </li>
   <li>
    <b>Beamforming</b>: 
    <br><a href="https://web.stanford.edu/~boyd/papers/pdf/rmvb.pdf">Robust Minimum Variance Beamforming</a>
   </li>
   <li>
    <b> Graphical models</b>: 
    <br><a href="http://haimbar.github.io/BlessingOfDimensionalityWorkshop/files/MahoneyMartin.pdf">Implicit Self-Regularization in Deep Neural Networks: Evidence
from Random Matrix Theory and Implications for Learning</a>
    <br><a href="http://haimbar.github.io/BlessingOfDimensionalityWorkshop/files/abraham1994.pdf">The Application of Improved Covariance Estimation
to Adaptive Beamforming and Detection</a>
    <br><a href="http://haimbar.github.io/BlessingOfDimensionalityWorkshop/files/bar2023.pdf">On graphical models and convex geometry</a>
    <br><a href="http://haimbar.github.io/BlessingOfDimensionalityWorkshop/files/cox1973.pdf">Resolving power and sensitivity to mismatch of optimum
array processors</a>
    <br><a href="http://haimbar.github.io/BlessingOfDimensionalityWorkshop/files/gurdogan2022.pdf">Multiple Anchor Point Shrinkage for the Sample Covariance Matrix</a>
   </li>
  </ul>
 </p>


</div>


</body>


</html>
